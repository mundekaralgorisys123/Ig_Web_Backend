import time
import re
import os
from datetime import datetime
import requests
from playwright.sync_api import sync_playwright, TimeoutError
from openpyxl import Workbook
from openpyxl.drawing.image import Image
from flask import Flask
from bs4 import BeautifulSoup
import uuid
import logging
import base64
from dotenv import load_dotenv
from utils import get_public_ip, log_event, sanitize_filename
from database import insert_into_db, create_table
from limit_checker import update_product_count
# Load environment variables from .env file
load_dotenv()
# Get proxy URL from .env, if available
PROXY_URL = os.getenv("PROXY_URL")


app = Flask(__name__)

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
EXCEL_DATA_PATH = os.path.join(app.root_path, 'static', 'ExcelData')
IMAGE_SAVE_PATH = os.path.join(BASE_DIR, 'static', 'Images')

from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import time
import logging

def handle_tiffany(url, max_pages, final_url):
    ip_address = get_public_ip()
    logging.info(f"Scraping started for: {url} from IP: {ip_address} max_pages: {max_pages}")
    print("====================== IN =======================================================")

    with sync_playwright() as p:
        # browser = p.chromium.connect_over_cdp(PROXY_URL)
        browser = p.chromium.launch(headless=False)
        # Improved browser launch with stealth settings
        # browser = p.chromium.launch(
        # headless=True,
        # args=[
        #     "--no-sandbox",
        #     "--disable-blink-features=AutomationControlled",
        #     "--start-maximized"
        # ]
        # )
        
        # browser = p.chromium.launch(
        #     headless=True,
        #     args=[
        #         "--no-sandbox",
        #         "--disable-blink-features=AutomationControlled",
        #         "--start-maximized",
        #         f"--proxy-server={PROXY_URL}",  # âœ… Add proxy here
        #     ]
        # )
        context = browser.new_context(
                viewport={"width": 1920, "height": 1080},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
            )
        
        page = context.new_page()


        all_products = set()
        page_counter = 1  # Start with page 1
        
        while page_counter <= max_pages:
            # Update URL for each page
            if page_counter == 1:
                page_url = url  # First page doesn't need the `&page=` parameter
            else:
                page_url = f"{url}&sort=sort_order&order=desc&page={page_counter}"
            
            print(f"Opening page: {page_url}")
            # page.goto(page_url, timeout=120000)
            page.goto(page_url, timeout=180000, wait_until="domcontentloaded")
            time.sleep(3)  # Allow the page to load
            
            # Check if Cloudflare or CAPTCHA page is displayed
            if "Just a moment" in page.content() or "captcha" in page.content().lower():
                print("Bot detection triggered. Stopping pagination.")
                break
            
            # Extract products from the current page
            product_elements = page.locator('.product-item').all()
            for product_element in product_elements:
                product_html = product_element.inner_html()
                product_soup = BeautifulSoup(product_html, 'html.parser')
                all_products.add(str(product_soup))  # Store as a string to maintain set uniqueness

            print(f"Products found on page {page_counter}: {len(product_elements)}")
            
            # Check if no new products are found
            if len(product_elements) == 0:
                print("No more products found. Stopping pagination.")
                break

            # Increment the page counter to go to the next page
            page_counter += 1
            time.sleep(2)  # Randomized sleep to mimic human behavior
        
        print(f"Total unique products found: {len(all_products)}")

        # Extract product data using BeautifulSoup
        content = BeautifulSoup(page.content(), 'html.parser')
        title_tag = content.find('title')
        page_title = title_tag.get_text() if title_tag else "No title found"
        print(f"Page Title: {page_title}")

        browser.close()

    # Convert HTML strings back to BeautifulSoup objects for further processing
    all_products = [BeautifulSoup(product, 'html.parser') for product in all_products]

    # Save the products to Excel
    return save_to_excel(all_products, page_title)





def save_to_excel(all_products, page_title):
    if not os.path.exists(EXCEL_DATA_PATH):
        os.makedirs(EXCEL_DATA_PATH)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    image_folder = os.path.join(IMAGE_SAVE_PATH, timestamp)
    os.makedirs(image_folder, exist_ok=True)

    wb = Workbook()
    sheet = wb.active
    sheet.title = "Products"

    headers = ["Current Date", "Header", "Product Name", "Image", "Kt", "Price", "Total Dia wt", "Time", "ImagePath"]
    sheet.append(headers)

    current_date = datetime.now().strftime("%Y-%m-%d")
    time_only = datetime.now().strftime("%H.%M")

    # Ensure table exists
    # create_table()

    def product_generator():
        row_num = 2
        for product in all_products:
            unique_id = str(uuid.uuid4())

            product_name_tag = product.find('div', class_='clp-hover-info')
            product_name = product_name_tag.find('a').text.strip() if product_name_tag and product_name_tag.find('a') else "N/A"

            product_price_tag = product.find('span', class_='price')
            price = product_price_tag.text.strip() if product_price_tag else "N/A"

            image_tag = product.find('div', class_='category-product-images')
            image_url = image_tag.find('img')['data-src'] if image_tag and image_tag.find('img') else "N/A"

            metal_type_tag = product.find('span', class_='metal-type')
            kt = metal_type_tag.text.strip() if metal_type_tag else "N/A"

            diamond_weight_match = re.search(r"(\d+(\.\d+)?)\s*(ct|tcw)", product_name)
            diamond_weight = diamond_weight_match.group() if diamond_weight_match else "N/A"

            image_path = None  # Initialize variable

            # Save Image
            if image_url != "N/A":
                try:
                    img_data = requests.get(image_url).content
                    image_filename = f"{sanitize_filename(product_name)}_{timestamp}.jpg"
                    image_full_path = os.path.join(image_folder, image_filename)

                    with open(image_full_path, "wb") as f:
                        f.write(img_data)

                    image_path = os.path.join("Images", timestamp, image_filename)

                    # Insert Image in Excel
                    img = Image(image_full_path)
                    img.width, img.height = 100, 100
                    sheet.add_image(img, f"D{row_num}")

                except Exception as e:
                    log_event(f"Error downloading image for {product_name}: {e}")

            # Insert into Excel
            sheet.append([current_date, page_title, product_name, None, kt, price, diamond_weight, time_only, image_url])
            row_num += 1

            yield (unique_id, current_date, page_title, product_name, image_path, kt, price, diamond_weight)

    # Insert products using a generator (memory-efficient)
    insert_into_db(list(product_generator()))

    # Save Excel File
    filename = f"tiffany_{current_date}_{time_only}.xlsx"
    file_path = os.path.join(EXCEL_DATA_PATH, filename)
    wb.save(file_path)
    log_event(f"Data saved to {file_path}")

    # Encode Excel File
    with open(file_path, "rb") as file:
        base64_encoded = base64.b64encode(file.read()).decode("utf-8")
    print("====================== OUT =======================================================")
    
    products_fetched = len(all_products)
    # Update the product count.
    update_product_count(products_fetched)
        
    return base64_encoded, filename, file_path